{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建C4.5决策树\n",
    "## 目录\n",
    "* C4.5决策树简介\n",
    "* 实现C4.5决策树的版本1\n",
    "* C4.5相对于ID3算法的改进之处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5决策树简介\n",
    "C4.5决策树是在ID3算法的基础上进行改进，主要是改进两个部分\n",
    "1. 使用信息增益率（Gain Ratio）作为选取切分字段的参考指标\n",
    "$$Gain Ratio = \\frac{Information Gain}{Information Value}$$\n",
    "其中$$Information Values = -\\sum\\limits_{i=1}^k P(v_i)log_2P(v_i)$$\n",
    "$v_i$为子节点中样本量占父节点样本量的比例\n",
    "\n",
    "    我们选择 信息增益率最大的那一列，本质是信息增益最大，分支度又较小的列（也就是纯度提升很快，但又不是靠着把类别分 特别细 来提升 那些特征）\n",
    "\n",
    "2. 添加连续变量处理手段\n",
    "\t如果输入特征字段是连续性变量，则有下列步骤：\n",
    "\t1. 算法首先对这一列数从小到大排序\n",
    "\t2. 选取相邻的两个数的中间数作为切分数据集的备选点，此时针对连续变量的处理并非将其转化为一个拥有N-1个分类水平的分类变量，而是将其转化为N-1个二分方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现C4.5决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现过程中，基于原本的ID3决策树的代码进行修改，具体修改内容：\n",
    "\n",
    "1. 需要对特征字段是连续性变量还是离散型变量进行判断，故在原类的基础上，添加判断特征字段类别的预处理\n",
    "2. 采用信息增益率，同时，连续性变量有对应不同的操作，故bestsplit函数需要进行调整\n",
    "3. 对连续性变量采用二分的方法，故splitData函数需要进行调整\n",
    "4. 建树时，对于连续性变量，不再是以某个类别标签来进行划分，故createTree函数需要进行调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01版本\n",
    "感觉比较复杂，所以就先一点一点来。  \n",
    "先完成基于信息增益率来划分的改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from graphviz import Digraph\n",
    "import graphviz\n",
    "class Decision_tree():\n",
    "    \n",
    "    def __init__(self,cal = \"Entropy\"):\n",
    "        self.tree = None\n",
    "        self.cal = cal\n",
    "        self.columns=None\n",
    "    \n",
    "    def calEnt(self,y):\n",
    "        count = Counter(y)\n",
    "        p = np.array(list(count.values()))/len(y)\n",
    "        Ent = (-p* np.log2(p)).sum()\n",
    "        return Ent\n",
    "    \n",
    "    def calGini(self,y):\n",
    "        count = Counter(y)\n",
    "        p = np.array(list(count.values()))/len(y)\n",
    "        Gini = 1-(p**2).sum()\n",
    "        return Gini\n",
    "    \n",
    "    def calimpurity(self,y):\n",
    "        \"\"\"根据参数来选择是使用Entropy还是Gini来计算不纯度\"\"\"\n",
    "        if self.cal == \"Entropy\":\n",
    "            return self.calEnt(y)\n",
    "        else:\n",
    "            return self.calGini(y)\n",
    "    \n",
    "    \n",
    "    def fit_C45_01(self,X,y,featurename):\n",
    "        \n",
    "        def bestSplit(X,y):\n",
    "            \"\"\"让数据集根据某一个特征值进行划分，返回数据集最佳切分列索引\"\"\"\n",
    "            bestFeature=-1\n",
    "            bestGainRate=-1\n",
    "            baseEnt = self.calimpurity(y)\n",
    "            \n",
    "            for i in range(X.shape[1]):\n",
    "                label = list(Counter(X[:,i]).keys())\n",
    "                sub_ent = 0\n",
    "                sub_info = 0 #计算information value\n",
    "                for j in label:\n",
    "                    subData = y[X[:,i] == j]\n",
    "                    childEnt = self.calimpurity(subData)\n",
    "                    sub_ent += childEnt*len(subData)/len(y)\n",
    "                    sub_info += -len(subData)/len(y)*np.log2(len(subData)/len(y))\n",
    "                Gain = baseEnt - sub_ent\n",
    "                Gainrate = Gain/sub_info\n",
    "                if Gainrate > bestGainRate: \n",
    "                    bestGainRate = sub_ent\n",
    "                    bestFeature = i\n",
    "            return bestFeature\n",
    "        \n",
    "        \n",
    "        def splitData(X,y,feature,label):\n",
    "            \"\"\"按指定的特征和标签来划分数据子集\"\"\"\n",
    "            subX = X[X[:,feature] == label]\n",
    "            suby = y[X[:,feature] == label]\n",
    "            subX = np.delete(subX,feature,axis=1)\n",
    "            return subX,suby\n",
    "        \n",
    "        \n",
    "        def createTree(X,y,featurename):\n",
    "            \"\"\"用字典的形式保存最终的树\"\"\"\n",
    "            if X.shape[1]==1 or len(list(Counter(y)))==1:#即没有再可以划分的特征，或者子集已经只有一列，则迭代结束\n",
    "                return Counter(y).most_common(1)[0][0]#返回所占比例最多的类别\n",
    "            \n",
    "            bestfeature = bestSplit(X,y)\n",
    "            bestfeaturename = featurename[bestfeature]\n",
    "            labellist = set(Counter(X[:,bestfeature]))\n",
    "            dic = {}\n",
    "            for label in labellist:\n",
    "                subX,suby = splitData(X,y,bestfeature,label)\n",
    "                col = featurename.copy()\n",
    "                del col[bestfeature]\n",
    "                \n",
    "                dic[label] = createTree(subX,suby,col)\n",
    "            mytree = {bestfeaturename:dic}\n",
    "            return mytree\n",
    "         \n",
    "        self.columns = featurename    \n",
    "        self.tree = createTree(X,y,featurename)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _predict(self,test):\n",
    "        \"\"\"对单条测试集进行预测\"\"\"\n",
    "\n",
    "        def __predict(tree,test,columns):\n",
    "            feature = next(iter(tree))\n",
    "            secondDic = tree[feature]\n",
    "            index = columns.index(feature)\n",
    "            content = test[index]\n",
    "            for key in secondDic:\n",
    "                if key == content:\n",
    "                    if type(secondDic[key]) == dict :\n",
    "                        return __predict(secondDic[key],test,columns)\n",
    "                    else:\n",
    "                        return secondDic[key]\n",
    "\n",
    "        assert self.tree is not None,\"fit before predict\"\n",
    "        tree = self.tree\n",
    "        columns = self.columns\n",
    "        return __predict(tree,test,columns)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.array([self._predict(test) for test in X_test])\n",
    "            \n",
    "    def score(self,X_test,y_test):\n",
    "        \"\"\"计算模型的准确率\"\"\"\n",
    "        y_predict = self.predict(X_test)\n",
    "        return (y_test == y_predict).mean()\n",
    "    \n",
    "    def draw_tree(self):\n",
    "        from graphviz import Digraph\n",
    "        \n",
    "        def export_graphviz(tree,root_index): \n",
    "            root = next(iter(tree))\n",
    "            text_node.append([str(root_index),root])\n",
    "            secondDic = tree[root]\n",
    "            for key in secondDic:\n",
    "                if type(secondDic[key]) == dict:\n",
    "                    i[0]+=1\n",
    "                    secondrootindex=i[0]\n",
    "                    text_edge.append([str(root_index),str(secondrootindex),str(key)])\n",
    "                    export_graphviz(secondDic[key],secondrootindex)\n",
    "                else:\n",
    "                    i[0] += 1\n",
    "                    text_node.append([str(i[0]),str(secondDic[key])])\n",
    "                    text_edge.append([ str(root_index) , str(i[0]) , str(key) ])\n",
    "          \n",
    "        \n",
    "        tree = self.tree\n",
    "        text_node=[]\n",
    "        text_edge=[]\n",
    "        i=[1]\n",
    "        export_graphviz(tree,i[0])\n",
    "        dot = Digraph()\n",
    "        for line in text_node:\n",
    "            dot.node(line[0],line[1])\n",
    "        for line in text_edge:\n",
    "            dot.edge(line[0],line[1],line[2])\n",
    "        \n",
    "        dot.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用数据集进行验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([\n",
    "    [1,\"<=30\",\"high\",\"no\",\"fair\",\"no\"],\n",
    "    [2,\"<=30\",\"high\",\"no\",\"excellent\",\"no\"],\n",
    "    [3,\"31~40\",\"high\",\"no\",\"fair\",\"yes\"],\n",
    "    [4,\">40\",\"medium\",\"no\",\"fair\",\"yes\"],\n",
    "    [5,\">40\",\"low\",\"yes\",\"fair\",\"yes\"],\n",
    "    [6,\">40\",\"low\",\"yes\",\"excellent\",\"no\"],\n",
    "    [7,\"31~40\",\"low\",\"yes\",\"excellent\",\"yes\"],\n",
    "    [8,\"<=30\",\"medium\",\"no\",\"fair\",\"no\"],\n",
    "    [9,\"<=30\",\"low\",\"yes\",\"fair\",\"yes\"],\n",
    "    [10,\">40\",\"medium\",\"yes\",\"fair\",\"yes\"],\n",
    "    [11,\"<=30\",\"medium\",\"yes\",\"excellent\",\"yes\"],\n",
    "    [12,\"31~40\",\"medium\",\"no\",\"excellent\",\"yes\"],\n",
    "    [13,\"31~40\",\"high\",\"yes\",\"fair\",\"yes\"],\n",
    "    [14,\">40\",\"medium\",\"no\",\"excellent\",\"no\"]\n",
    "                 ],columns=[\"index\",\"age\",\"income\",\"student\",\"credit_rating\",\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': {'>40': {'credit_rating': {'excellent': 'no', 'fair': 'yes'}}, '<=30': {'credit_rating': {'excellent': {'student': {'no': 'no', 'yes': 'yes'}}, 'fair': {'student': {'no': 'no', 'yes': 'yes'}}}}, '31~40': 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "clf = Decision_tree()\n",
    "clf.fit_C45_01(np.array(X),np.array(y),list(X.columns))\n",
    "print(clf.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](terribletree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比之前使用ID3算法构建的决策树，现在C4.5构建的树简直好太多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02版本\n",
    "添加决策树对于连续性变量的分类  \n",
    "由于这部分基本上就是在CART树的基础上，添加对离散变量的多分枝处理，较为复杂\n",
    "而且对于离散型变量来说，本身也是可以全部按照二分法将其分离，故暂时就不实现这个版本的C4.5树啦\n",
    "\n",
    "之后有兴趣了，可以在CART树的代码基础上进行修改。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
