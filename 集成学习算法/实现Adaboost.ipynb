{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boost集成基分类器的方式是通过每一轮调整训练数据的权重或概率分布，将弱分类器组和成一个强分类器  \n",
    "对于Adaboost来说，  \n",
    "每一次提高前一轮被若分类器错误分类的样本的权重，降低那些被正确分类的样本权重。  \n",
    "最终的分类器是这些基分类器的线性组合，其中分类误差率小的权重大，分类误差大的权重小\n",
    "\n",
    "Adaboost最终的分类器为\n",
    "$G(x) = sign(f(x))=sign\\big(\\sum\\limits_{m=1}^M\\alpha_mG_m(x)\\big)$  \n",
    "其中$\\alpha_m= \\frac{1}{2}log(\\frac{1-e_m}{e_m})$\n",
    "e为错误率\n",
    "\n",
    "每次样本的权重是如何改变的呢？  \n",
    "第一轮的时候，训练数据的权重分布为1/N  \n",
    "当完成m轮训练后\n",
    "$$D_{m+1} = (w_{m+1,1},w_{m+1,2},...w_{m+1,N})$$ \n",
    "$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}exp(-\\alpha_my_iGm(x_i))$$\n",
    "上面的式子还可以改写成\n",
    "$$\n",
    "w_{m+1,i}=\\begin{cases}\\frac{w_{m,i}}{Z_m}e^{-\\alpha_m} ,& Gm(x_i)=y_i\\\\\\frac{w_{m,i}}{Z_m}e^{\\alpha_m} ,& Gm(x_i)\\not=y_i\\end{cases}\n",
    "$$\n",
    "也就是对于G_m分类器来说,如果分类正确，则在下一次分类时，减小这个样本对应的权重，如果分类错误，则在下一次分类时，增加这个样本对应的权重  \n",
    "其中$Z_m$是规范化因子，让算出来的$w_{m+1,i}$的总和为1  \n",
    "故$Z_m=\\sum\\limits_{i=1}^Nw_{m,i}exp(-\\alpha_my_iGm(x_i))$  \n",
    "或者简单理解为$sum(D_{m+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法过程\n",
    "1. 初始化训练数据的权值分布\n",
    "2. 对m=1,2,...,M  \n",
    "    a. 使用具有权值分布的Dm训练数据集学习，得到基本分类器Gm  \n",
    "    b. 计算Gm(x)在训练数据集上的分类误差（注意测试的训练集是有权值的）  \n",
    "    c. 计算Gm(x)对应的系数，即$\\alpha_m$  \n",
    "    d. 更新训练数据集的权值分布  \n",
    "    结束的标志为构建的f(x)的错误率为0或者弱分类器的个数达到指定值\n",
    "3. 构建基本分类器的线性组合\n",
    "    $$f(x)=\\sum\\limits_{m=1}^M\\alpha_mG_m(x)$$\n",
    "    最终的分类器为$G(x) = sign(f(x))=sign(\\sum\\limits_{m=1}^M\\alpha_mG_m(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于单层决策树构建若分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单层决策树又名决策树桩，只有一个决策分枝，以某一个维度进行划分。\n",
    "在构建单层决策树的时候，在所有的特征中进行便利，选择加权错误率最低的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data=np.array([[1.0,2.1,1.0],\n",
    "                [1.5,1.6,1.0],\n",
    "               [1.3,1.0,-1.0],\n",
    "               [1.0,1.0,-1.0],\n",
    "               [2.0,1.0,1.0]]\n",
    "             )\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[y==1,0],X[y==1,1],c='blue')\n",
    "plt.scatter(X[y==-1,0],X[y==-1,1],c='red')\n",
    "plt.xticks(X[:,0])\n",
    "plt.yticks(X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_stump(X,y,D):\n",
    "    minerror = float(\"inf\")\n",
    "    bestsplit = -1 #最佳分割点\n",
    "    bestlabel = -1 #最佳分割特征\n",
    "    beststump={}\n",
    "    for label in range(X.shape[1]):\n",
    "        value = np.append(X[:,label],np.array([X[:,label].min()-1,X[:,label].max()+1]))\n",
    "        index = np.argsort(value)\n",
    "        for i in range(1,len(index)):\n",
    "            if value[index[i-1]] != value[index[i]]:\n",
    "                split = (value[index[i-1]] + value[index[i]])/2\n",
    "                indexleft = X[:,label] < split\n",
    "                for _ in [1,-1]:#依此假设划分的左侧的决策为-1，+1\n",
    "                    res = np.ones(len(y))\n",
    "                    res[indexleft] = -1\n",
    "                    res = res*_\n",
    "                    err = np.sum((res != y)*D)\n",
    "                    if err < minerror:\n",
    "                        minerror = err\n",
    "                        bestlabel = label\n",
    "                        bestsplit = split\n",
    "                        symbol = _\n",
    "                        bestres = res\n",
    "                        \n",
    "    beststump[\"bestlabel\"]=bestlabel\n",
    "    beststump[\"bestsplit\"]=bestsplit\n",
    "    beststump[\"symbol\"]=symbol\n",
    "    return beststump,minerror,bestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bestlabel': 0, 'bestsplit': 1.4, 'symbol': 1},\n",
       " 0.2,\n",
       " array([-1.,  1., -1., -1.,  1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_stump(X,y,np.ones(len(y))/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost():\n",
    "    def __init__(self,n_estimator=10):\n",
    "        self.weakclass_ = []\n",
    "        self.alpha_ = np.array([])\n",
    "        self.n_estimator = n_estimator\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        def set_stump(X,y,D):\n",
    "            minerror = float(\"inf\")\n",
    "            bestsplit = -1 #最佳分割点\n",
    "            bestlabel = -1 #最佳分割特征\n",
    "            beststump={}\n",
    "            \n",
    "            for label in range(X.shape[1]):\n",
    "                value = np.append(X[:,label],np.array([X[:,label].min()-1,X[:,label].max()+1]))\n",
    "                index = np.argsort(value)\n",
    "                for i in range(1,len(index)):\n",
    "                    if value[index[i-1]] != value[index[i]]:\n",
    "                        split = (value[index[i-1]] + value[index[i]])/2\n",
    "                        indexleft = X[:,label] < split\n",
    "                        for _ in [1,-1]:#依此假设划分的左侧的决策为-1，+1\n",
    "                            res = np.ones(len(y))\n",
    "                            res[indexleft] = -1\n",
    "                            res = res*_\n",
    "                            err = np.sum((res != y)*D)\n",
    "                            if err < minerror:\n",
    "                                minerror = err\n",
    "                                bestlabel = label\n",
    "                                bestsplit = split\n",
    "                                symbol = _\n",
    "                                bestres = res\n",
    "                        \n",
    "            beststump[\"bestlabel\"]=bestlabel\n",
    "            beststump[\"bestsplit\"]=bestsplit\n",
    "            beststump[\"symbol\"]=symbol\n",
    "            return beststump,minerror,bestres\n",
    "        \n",
    "        \n",
    "        D = np.ones(len(y))/len(y)\n",
    "        ensembleresult = np.zeros(len(y))\n",
    "        for i in range(self.n_estimator):\n",
    "            print(\"D=\",D)\n",
    "            weakclass,err,res = set_stump(X,y,D)\n",
    "            alpha = 1/2*np.log((1-err)/max(err,1e-16)) #防止出现err=0的情形\n",
    "            self.weakclass_.append(weakclass)\n",
    "            self.alpha_ = np.append(self.alpha_,np.array(alpha))\n",
    "            print('alpha=',alpha)\n",
    "            D = D * np.exp(-1*alpha*y*res)\n",
    "            D = D/np.sum(D)\n",
    "            \n",
    "            ensembleresult += alpha * res #更新累计估计值\n",
    "            errrate = (np.sign(ensembleresult) != y).mean()\n",
    "            if errrate <1e-16:\n",
    "                return\n",
    "            \n",
    "    def predict(self,X_test):\n",
    "        res = np.empty(len(X_test))\n",
    "        for i in range(len(X_test)):\n",
    "            res[i] = self._predict(X_test[i])\n",
    "        return res\n",
    "    \n",
    "    def _predict(self,X_test_data):   \n",
    "        res = np.empty(len(self.weakclass_))\n",
    "        for i in range(len(self.weakclass_)):\n",
    "            label = self.weakclass_[i][\"bestlabel\"]\n",
    "            symbol = self.weakclass_[i][\"symbol\"]\n",
    "            split = self.weakclass_[i][\"bestsplit\"]\n",
    "            res[i] = symbol*int(1 if X_test_data[label] > split else -1)\n",
    "        return np.sign((res*self.alpha_).sum())\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        pre = self.predict(X_test)\n",
    "        return (pre==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D= [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01]\n",
      "alpha= 1.0453705484668847\n",
      "D= [0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.04545455 0.04545455 0.00561798 0.00561798\n",
      " 0.04545455 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.04545455 0.00561798 0.00561798\n",
      " 0.04545455 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.04545455 0.00561798 0.04545455\n",
      " 0.04545455 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.04545455 0.00561798 0.00561798 0.00561798 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.00561798 0.04545455 0.00561798 0.00561798\n",
      " 0.00561798 0.00561798 0.04545455 0.00561798]\n",
      "alpha= 1.1575038064963017\n",
      "D= [0.00308642 0.03125    0.00308642 0.00308642 0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.03125    0.00308642 0.00308642 0.00308642\n",
      " 0.03125    0.03125    0.02497194 0.02497194 0.00308642 0.00308642\n",
      " 0.02497194 0.00308642 0.00308642 0.00308642 0.00308642 0.00308642\n",
      " 0.00308642 0.03125    0.00308642 0.00308642 0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.00308642 0.02497194 0.00308642 0.00308642\n",
      " 0.02497194 0.00308642 0.03125    0.00308642 0.00308642 0.03125\n",
      " 0.00308642 0.00308642 0.00308642 0.03125    0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.03125    0.03125    0.03125    0.00308642\n",
      " 0.00308642 0.00308642 0.03125    0.02497194 0.00308642 0.02497194\n",
      " 0.02497194 0.00308642 0.00308642 0.00308642 0.00308642 0.03125\n",
      " 0.00308642 0.00308642 0.00308642 0.00308642 0.03125    0.00308642\n",
      " 0.00308642 0.00308642 0.00308642 0.00308642 0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.00308642 0.00308642 0.00308642 0.00308642\n",
      " 0.02497194 0.03125    0.03125    0.00308642 0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.00308642 0.02497194 0.00308642 0.00308642\n",
      " 0.00308642 0.00308642 0.02497194 0.00308642]\n",
      "alpha= 0.6764789704261496\n",
      "D= [0.00194209 0.01966367 0.00194209 0.00194209 0.00194209 0.00194209\n",
      " 0.00194209 0.00194209 0.01966367 0.00194209 0.00194209 0.00194209\n",
      " 0.01966367 0.01966367 0.01571328 0.01571328 0.00194209 0.00194209\n",
      " 0.01571328 0.00194209 0.00194209 0.00194209 0.00194209 0.00194209\n",
      " 0.00194209 0.01966367 0.00194209 0.00194209 0.00194209 0.00194209\n",
      " 0.00194209 0.00194209 0.00194209 0.01571328 0.00194209 0.00194209\n",
      " 0.01571328 0.00194209 0.01966367 0.00194209 0.00194209 0.01966367\n",
      " 0.00194209 0.00194209 0.00194209 0.01966367 0.00194209 0.00194209\n",
      " 0.00194209 0.00194209 0.01966367 0.01966367 0.01966367 0.00751366\n",
      " 0.00194209 0.00751366 0.01966367 0.06079235 0.00194209 0.06079235\n",
      " 0.06079235 0.00194209 0.00194209 0.00194209 0.00751366 0.01966367\n",
      " 0.00751366 0.00751366 0.00194209 0.00751366 0.01966367 0.00194209\n",
      " 0.00194209 0.00194209 0.00194209 0.00194209 0.00194209 0.00194209\n",
      " 0.00194209 0.00751366 0.00751366 0.00751366 0.00751366 0.00194209\n",
      " 0.06079235 0.01966367 0.01966367 0.00194209 0.00751366 0.00751366\n",
      " 0.00751366 0.00194209 0.00751366 0.06079235 0.00751366 0.00751366\n",
      " 0.00751366 0.00194209 0.06079235 0.00751366]\n",
      "alpha= 0.7916446000394883\n",
      "D= [0.00570096 0.05772221 0.0011704  0.0011704  0.00570096 0.00570096\n",
      " 0.0011704  0.00570096 0.0118503  0.00570096 0.00570096 0.0011704\n",
      " 0.0118503  0.0118503  0.04612594 0.04612594 0.00570096 0.00570096\n",
      " 0.04612594 0.00570096 0.00570096 0.00570096 0.0011704  0.00570096\n",
      " 0.0011704  0.05772221 0.00570096 0.00570096 0.00570096 0.0011704\n",
      " 0.0011704  0.00570096 0.00570096 0.04612594 0.00570096 0.00570096\n",
      " 0.04612594 0.00570096 0.0118503  0.00570096 0.00570096 0.0118503\n",
      " 0.0011704  0.00570096 0.00570096 0.0118503  0.00570096 0.0011704\n",
      " 0.00570096 0.00570096 0.0118503  0.0118503  0.0118503  0.0045281\n",
      " 0.0011704  0.0045281  0.0118503  0.03663647 0.0011704  0.03663647\n",
      " 0.03663647 0.0011704  0.0011704  0.0011704  0.0045281  0.0118503\n",
      " 0.0045281  0.0045281  0.0011704  0.0045281  0.0118503  0.0011704\n",
      " 0.0011704  0.0011704  0.0011704  0.0011704  0.0011704  0.0011704\n",
      " 0.0011704  0.0045281  0.0045281  0.0045281  0.0045281  0.0011704\n",
      " 0.03663647 0.0118503  0.0118503  0.0011704  0.0045281  0.0045281\n",
      " 0.0045281  0.0011704  0.0045281  0.03663647 0.0045281  0.0045281\n",
      " 0.0045281  0.0011704  0.03663647 0.0045281 ]\n",
      "alpha= 0.7808413237271861\n",
      "D= [0.00344846 0.03491567 0.00070796 0.00070796 0.00344846 0.00344846\n",
      " 0.00070796 0.00344846 0.03416935 0.00344846 0.00344846 0.00070796\n",
      " 0.00716814 0.00716814 0.02790118 0.02790118 0.00344846 0.00344846\n",
      " 0.02790118 0.00344846 0.00344846 0.00344846 0.00070796 0.00344846\n",
      " 0.00070796 0.03491567 0.00344846 0.00344846 0.00344846 0.00070796\n",
      " 0.00070796 0.00344846 0.00344846 0.02790118 0.00344846 0.00344846\n",
      " 0.02790118 0.00344846 0.00716814 0.00344846 0.00344846 0.03416935\n",
      " 0.00070796 0.00344846 0.00344846 0.00716814 0.00344846 0.00070796\n",
      " 0.00344846 0.00344846 0.03416935 0.03416935 0.03416935 0.00273901\n",
      " 0.00070796 0.00273901 0.03416935 0.02216109 0.00070796 0.02216109\n",
      " 0.02216109 0.00337475 0.00070796 0.00070796 0.00273901 0.03416935\n",
      " 0.01305641 0.00273901 0.00070796 0.00273901 0.03416935 0.00070796\n",
      " 0.00070796 0.00070796 0.00070796 0.00337475 0.00070796 0.00337475\n",
      " 0.00070796 0.00273901 0.00273901 0.00273901 0.00273901 0.00070796\n",
      " 0.10563825 0.03416935 0.03416935 0.00070796 0.01305641 0.00273901\n",
      " 0.00273901 0.00337475 0.00273901 0.02216109 0.00273901 0.01305641\n",
      " 0.00273901 0.00070796 0.02216109 0.00273901]\n",
      "alpha= 0.6590116488550704\n",
      "D= [0.00218574 0.08268081 0.00167647 0.00167647 0.00218574 0.00218574\n",
      " 0.00167647 0.00816601 0.08091352 0.00816601 0.00218574 0.00167647\n",
      " 0.01697426 0.01697426 0.01768466 0.01768466 0.00218574 0.00218574\n",
      " 0.01768466 0.00218574 0.00816601 0.00218574 0.00044873 0.00816601\n",
      " 0.00167647 0.08268081 0.00816601 0.00218574 0.00816601 0.00167647\n",
      " 0.00167647 0.00816601 0.00218574 0.01768466 0.00816601 0.00816601\n",
      " 0.01768466 0.00218574 0.01697426 0.00816601 0.00218574 0.08091352\n",
      " 0.00167647 0.00218574 0.00218574 0.01697426 0.00218574 0.00167647\n",
      " 0.00218574 0.00816601 0.02165763 0.02165763 0.02165763 0.00173607\n",
      " 0.00044873 0.00173607 0.02165763 0.0140464  0.00044873 0.0140464\n",
      " 0.0140464  0.00213902 0.00044873 0.00044873 0.00173607 0.02165763\n",
      " 0.00827557 0.00173607 0.00044873 0.00173607 0.02165763 0.00044873\n",
      " 0.00044873 0.00044873 0.00044873 0.00213902 0.00044873 0.00213902\n",
      " 0.00044873 0.00173607 0.00173607 0.00173607 0.00173607 0.00044873\n",
      " 0.0669569  0.02165763 0.02165763 0.00044873 0.00827557 0.00173607\n",
      " 0.00173607 0.00213902 0.00173607 0.0140464  0.00173607 0.00827557\n",
      " 0.00173607 0.00044873 0.0140464  0.00173607]\n",
      "alpha= 0.7529588756433875\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost()\n",
    "# clf.fit(np.arange(0,10).reshape(-1,1),np.array([1,1,1,-1,-1,-1,1,1,1,-1]))\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1., -1.,  1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以鸢尾花数据集来测试手写的Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1159faa90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X = X[y!=2,:2]\n",
    "y = y[y!=2]\n",
    "y[y==0] = -1\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "plt.scatter(X[y==1,0],X[y==1,1],color=\"blue\")\n",
    "plt.scatter(X[y==-1,0],X[y==-1,1],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D= [0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333 0.01333333\n",
      " 0.01333333 0.01333333 0.01333333]\n",
      "alpha= 1.1367987780603968\n",
      "D= [0.00735294 0.00735294 0.07142857 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.07142857 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.07142857\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.07142857 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.07142857 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.07142857 0.00735294\n",
      " 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294 0.00735294\n",
      " 0.00735294 0.07142857 0.00735294]\n",
      "alpha= 1.1676874579085184\n",
      "D= [0.00403226 0.00403226 0.03917051 0.00403226 0.00403226 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.04166667 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.00403226 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.03917051 0.04166667\n",
      " 0.00403226 0.00403226 0.00403226 0.04166667 0.00403226 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.00403226 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.00403226 0.03917051\n",
      " 0.00403226 0.00403226 0.00403226 0.00403226 0.03917051 0.00403226\n",
      " 0.00403226 0.00403226 0.00403226 0.04166667 0.00403226 0.04166667\n",
      " 0.00403226 0.00403226 0.04166667 0.00403226 0.03917051 0.00403226\n",
      " 0.04166667 0.04166667 0.00403226 0.00403226 0.03917051 0.04166667\n",
      " 0.04166667 0.04166667 0.00403226 0.00403226 0.00403226 0.04166667\n",
      " 0.00403226 0.03917051 0.00403226]\n",
      "alpha= 0.9296158584831895\n",
      "D= [0.00233023 0.00233023 0.02263648 0.00233023 0.00233023 0.00233023\n",
      " 0.01495726 0.01495726 0.00233023 0.01495726 0.02407901 0.00233023\n",
      " 0.01495726 0.00233023 0.01495726 0.01495726 0.00233023 0.01495726\n",
      " 0.00233023 0.00233023 0.00233023 0.00233023 0.02263648 0.02407901\n",
      " 0.01495726 0.00233023 0.00233023 0.02407901 0.00233023 0.00233023\n",
      " 0.00233023 0.00233023 0.00233023 0.00233023 0.01495726 0.01495726\n",
      " 0.00233023 0.00233023 0.01495726 0.00233023 0.00233023 0.02263648\n",
      " 0.00233023 0.00233023 0.00233023 0.01495726 0.02263648 0.00233023\n",
      " 0.00233023 0.01495726 0.00233023 0.02407901 0.00233023 0.02407901\n",
      " 0.00233023 0.00233023 0.02407901 0.01495726 0.14529915 0.00233023\n",
      " 0.02407901 0.02407901 0.00233023 0.00233023 0.14529915 0.02407901\n",
      " 0.02407901 0.02407901 0.00233023 0.00233023 0.00233023 0.02407901\n",
      " 0.00233023 0.02263648 0.00233023]\n",
      "alpha= 0.8007348713924618\n",
      "D= [0.00694444 0.0014     0.0136     0.0014     0.0014     0.0014\n",
      " 0.00898632 0.00898632 0.0014     0.00898632 0.07175926 0.0014\n",
      " 0.00898632 0.0014     0.00898632 0.00898632 0.0014     0.00898632\n",
      " 0.0014     0.0014     0.0014     0.0014     0.0136     0.07175926\n",
      " 0.00898632 0.00694444 0.00694444 0.01446667 0.0014     0.0014\n",
      " 0.0014     0.0014     0.0014     0.0014     0.00898632 0.00898632\n",
      " 0.0014     0.00694444 0.00898632 0.00694444 0.0014     0.0136\n",
      " 0.0014     0.00694444 0.0014     0.00898632 0.0136     0.0014\n",
      " 0.0014     0.00898632 0.0014     0.01446667 0.0014     0.01446667\n",
      " 0.0014     0.00694444 0.01446667 0.00898632 0.08729573 0.00694444\n",
      " 0.01446667 0.07175926 0.0014     0.0014     0.08729573 0.07175926\n",
      " 0.07175926 0.01446667 0.0014     0.00694444 0.00694444 0.07175926\n",
      " 0.0014     0.0136     0.0014    ]\n",
      "alpha= 0.8362328024459018\n",
      "D= [0.00412425 0.00083145 0.04301192 0.00083145 0.00083145 0.00083145\n",
      " 0.0053369  0.0053369  0.00083145 0.0053369  0.04261723 0.00083145\n",
      " 0.0053369  0.00083145 0.0053369  0.0053369  0.00083145 0.0053369\n",
      " 0.00083145 0.00083145 0.00083145 0.00083145 0.04301192 0.04261723\n",
      " 0.0053369  0.00412425 0.00412425 0.00859163 0.00083145 0.00083145\n",
      " 0.00083145 0.00083145 0.00083145 0.00083145 0.0053369  0.0053369\n",
      " 0.00083145 0.00412425 0.0053369  0.00412425 0.00083145 0.04301192\n",
      " 0.00083145 0.00412425 0.00083145 0.0053369  0.04301192 0.00083145\n",
      " 0.00083145 0.0053369  0.00083145 0.00859163 0.00083145 0.00859163\n",
      " 0.00083145 0.00412425 0.00859163 0.0053369  0.05184421 0.00412425\n",
      " 0.00859163 0.04261723 0.00083145 0.0044277  0.27608503 0.04261723\n",
      " 0.04261723 0.00859163 0.00083145 0.00412425 0.00412425 0.04261723\n",
      " 0.0044277  0.04301192 0.00083145]\n",
      "alpha= 1.0318664120481371\n",
      "D= [0.00232397 0.00046851 0.02423676 0.00046851 0.00046851 0.00046851\n",
      " 0.00300729 0.02368335 0.00046851 0.00300729 0.02401436 0.00046851\n",
      " 0.00300729 0.00368968 0.00300729 0.02368335 0.00046851 0.02368335\n",
      " 0.00046851 0.00046851 0.00046851 0.00046851 0.02423676 0.02401436\n",
      " 0.00300729 0.00232397 0.00232397 0.03812672 0.00046851 0.00046851\n",
      " 0.00368968 0.00046851 0.00046851 0.00046851 0.00300729 0.00300729\n",
      " 0.00046851 0.00232397 0.00300729 0.00232397 0.00046851 0.02423676\n",
      " 0.00046851 0.00232397 0.00046851 0.00300729 0.02423676 0.00046851\n",
      " 0.00046851 0.00300729 0.00046851 0.03812672 0.00046851 0.03812672\n",
      " 0.00046851 0.00232397 0.03812672 0.00300729 0.02921367 0.00232397\n",
      " 0.03812672 0.02401436 0.00046851 0.00249496 0.15557103 0.18912062\n",
      " 0.02401436 0.03812672 0.00368968 0.00232397 0.00232397 0.02401436\n",
      " 0.00249496 0.02423676 0.00046851]\n",
      "alpha= 0.8137645961252398\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre != y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bestlabel': 2, 'bestsplit': 2.45, 'symbol': 1}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.weakclass_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
